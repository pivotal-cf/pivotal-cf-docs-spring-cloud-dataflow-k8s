---
title: Configuring Installation Values for Spring Cloud Data Flow for Kubernetes
owner: Spring Cloud Data Flow Release Engineering
---

This topic describes how to configure the Spring Cloud Data Flow for Kubernetes installation resources before deploying to the Kubernetes cluster.

Before proceeding, review the [Preparing to Install SCDF for Kubernetes](preparing-to-install-scdf-for-kubernetes.html) topic for details about:

* Preparing the Spring Cloud Data Flow for Kubernetes installation resources on your local workstation, and
* Installing required command-line tools

## <a id='overview'></a> Overview

There are a few concepts that drive how the configuration of Spring Cloud Data Flow for Kubernetes is performed.

* Support for multiple deployment environments
* Configuration of application properties
* Configuration of Kubernetes Resources
* Support for installation in an air-gapped environment

The following sections explain these concepts and the steps you need to take to configure them.


## <a id='support-for-multiple-environments'></a> Support for multiple deployment environments

The installation is based on the tool [Kustomize](https://kustomize.io) that allows you to customize Kubernetes resources by providing base resources files and patch files that modify the base resources to target different deployment environments.
Kustomize is built into the kubectl CLI.
The provided configuration files support a development and production environment.
The development environment is intended to quickly try Spring Cloud Data Flow for Kubernetes by provisioning a RabbitMQ message broker and Postresql database.


### <a id='directory-structure'></a> Directory Structure

The installation directory structure contains the following directories.

* `bin` - scripts for installing and installing to the development environment as well as a script for performing image relocation.
* `apps` - Kubernetes resource files and application configuration files to support deployment to multiple environments
* `services` - Kubernetes resources for deploying RabbitMQ and PostgreSQL fur use with the development environment

The apps directory contains the following folder structure based on Kustomize namving conventions

<pre>
.
├── apps
│   ├── data-flow
│   │   ├── images
│   │   ├── kustomize
│   │   │   ├── base
│   │   │   └── overlays
│   │   └── schemas
│   └── skipper
│       ├── images
│       ├── kustomize
│       │   ├── base
│       │   └── overlays
│       └── schemas
</pre>

There are two directories for each application that is part of Spring Cloud Data Flow for Kubernetes.
The first directory, data-flow, contains the Data Flow Server and the second directory contains the Skipper server.
Within each application directory there is the following directory structure

* `images` - location for container images that can be downloaded separately.
* `kustomize` - the directory containing Kubernetes and application configuration files for use with Kustomize.
* `schemas` - Database schemas that you can install manually if you do not want each server to install them upon startup.

The contents of the kustomize directory is shown below.
Edit files in this directory to configure the application and Kubernetes resources.

<pre>
.
├── base
│   ├── deployment.yaml
│   ├── kustomization.yaml
│   ├── role-binding.yaml
│   ├── roles.yaml
│   ├── service-account.yaml
│   └── service.yaml
└── overlays
    ├── dev
    │   ├── application.yaml
    │   ├── deployment-patch.yaml
    │   ├── kustomization.yaml
    │   └── service-patch.yaml
    └── production
        ├── application.yaml
        ├── deployment-patch.yaml
        ├── kustomization.yaml
        └── service-patch.yaml
</pre>

**Base**

The `base` directory contains kubernetes resource files with default configuration values that are then patched by files in the overlays directory.
The file `kustomization.yaml` references the other yaml files in that directory.

**Overlays**

There is a directory per deployment environment.
In this case `dev` and `production` environments.
In each directory there is a `kustomization.yaml` file that references the `kustomization.yaml` file in the base and adds additional patches to modify values unique to the target deployment environment.
You can also add additonal configuration in the `kustomization.yaml` to set the target namespace to deploy to add labels and name prefixes.  See the [kustomization documentation](https://github.com/kubernetes-sigs/kustomize/blob/master/docs/fields.md) for additional fields that can be configured in the kustomization.yaml file.


## <a id='configuration-steps'></a> Configuration Steps

The file `application.yaml` is a Spring Boot configuration file that you will edit to configure the Data Flow and Skipper server.
For example, you can configure `spring.datasource` properties or `spring.cloud.dataflow.features` properties.
The `deployment-patch.yaml` and `service-patch.yaml` files are where you will patch the base kubernetes resources with values appropriate for your target environment.
Examples of these fields are memory/cpu allocations and ingress configuration.


### <a id='database-configuration'></a> Database Configuration

You can customize the database configuration settings in the `application.yaml` and the `deployment-path.yaml` files.
These files are preconfigured to connect to a PostgreSQL database that can be installed either using the manifests provided in `services/dev/postgresql` or by installing the `bitnami/postgresql` Helm chart.

#### <a id='database-connection'></a> Database Connections Settings

To use a different database you would need to replace or remove the secret `volume` and `volumeMount` in the `deployment-patch.yaml` file.
If you replace the secret we recommned to provide `database-password` as the path for the password key since that simplifies the changes needed for the `application.yaml` file.

You need to change the database settings for both the `skipper` and `data-flow` apps.
Here is an example of using a secret named `production-db` with a `password` key (You can specify additional keys if they are available in the secret and reference the paths you specify in `application.yaml`):

```yaml
      containers:
      - name: skipper
        volumeMounts:
          - name: database
            mountPath: /etc/secrets/database
            readOnly: true
  ...

      volumes:
        - name: database
          secret:
            secretName: postgresql
            items:
            - key: postgresql-password
              path: database-password 

```

The settings in `application.yaml` that you need to consider are under `spring.datasource`, modify the `url`, `username`, `password` and `driverClassName` values to match you detabase settings:

```yaml
spring:

  ...

  datasource:
    url: jdbc:postgresql://database-host:5432/dataflow-db
    username: postgres
    password: ${database-password}
    driverClassName: org.postgresql.Driver
    testOnBorrow: true
    validationQuery: "SELECT 1"
```

#### <a id='database-connection'></a> Initializing Database Schema

You can initialize you database schema using the DDL scripts available under `apps/skipper/schemas` and `apps/data-flow/schemas`.
We provide DDL for DB2, MySQL, Oracle, PostgreSQL and MS SQL Server.
If there are multiple files they must be applied in the correct order starting with `V1-` followed by `V2-` and `V3-` etc.

If you initialize the schema for the database you can turn off the database initialization of the skipper and data-flow apps by adding the following settings to the respective `application.yaml` files:

```yaml
spring:

  ...

  jpa:
    hibernate:
      ddlAuto: none
  flyway:
    enabled: false
```

#### <a id='database-connection'></a> Adding JDBC Driver

It is possibe to add your own JDBC driver if you do not want to use the JDBC drivers that are provided.
JDBC drivers for DB2, MySQL, Oracle, PostgreSQL and MS SQL Server are provided in the published app artifacts.

In order to add a JDBC driver you need to perform the following steps:

1. Download the server jars for data-flow and skipper

    ```bash
    wget https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-skipper-server/2.4.1.RELEASE/spring-cloud-skipper-server-2.4.1.RELEASE.jar
    wget https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-dataflow-server/2.5.1.RELEASE/spring-cloud-dataflow-server-2.5.1.RELEASE.jar
    ```

1. Create a directory containing the JDBC driver to be added

    ```bash
    mkdir -p BOOT-INF/lib
    ```

    Then copy your JDBC sriver jar to this directory.

1. Add the driver to the downloaded jars

    ```bash
    jar -u0f spring-cloud-skipper-server-2.4.1.RELEASE.jar BOOT-INF/lib/*.jar
    jar -u0f spring-cloud-dataflow-server-2.5.1.RELEASE.jar BOOT-INF/lib/*.jar
    ```

1. Create Dockerfiles for adding the updated server jars as new container images

    Create two docker files:

    _Dockerfile-skipper-server_

    ```bash
    FROM springcloud/openjdk:2.0.1.RELEASE
    COPY spring-cloud-skipper-server-2.4.1.RELEASE.jar /maven/spring-cloud-skipper-server.jar
    ENTRYPOINT ["java", "-jar","/maven/spring-cloud-skipper-server.jar"]
    ```

    _Dockerfile-dataflow-server_

    ```bash
    FROM springcloud/openjdk:2.0.1.RELEASE
    COPY spring-cloud-dataflow-server-2.5.1.RELEASE.jar /maven/spring-cloud-dataflow-server.jar
    ENTRYPOINT ["java", "-jar","/maven/spring-cloud-dataflow-server.jar"]
    ```

1. Build the container images and push them to you registry

    Set the `REGISTRY` variable to the registry prefix you want to use.

    ```bash
    registry_prefix=registry.example.com/example
    docker build -t ${registry_prefix}/spring-cloud-skipper-server:2.4.1.RELEASE-jdbc -f Dockerfile-skipper-server .
    docker push ${registry_prefix}/spring-cloud-skipper-server:2.4.1.RELEASE-jdbc
    docker build -t ${registry_prefix}/spring-cloud-dataflow-server:2.5.1.RELEASE-jdbc -f Dockerfile-dataflow-server .
    docker push ${registry_prefix}/spring-cloud-dataflow-server:2.5.1.RELEASE-jdbc
    ```

1. Update your app kustomization manifests with the new image locations

    For skipper app modify `newName` and `newTag` values in `apps/skipper/kustomize/overlays/production/kustomization.yaml`

    ```bash
    images:
    - name: springcloud/spring-cloud-skipper-server  # used for Kustomize matching
      newName: registry.example.com/example/spring-cloud-skipper-server
      newTag: 2.4.1.RELEASE-jdbc
    configMapGenerator:
    - name: skipper
      files:
        - bootstrap.yaml
        - application.yaml
    bases:
      - ../../base
    patches:
    - deployment-patch.yaml
    - service-patch.yaml
    ```

    For data-flow app modify `newName` and `newTag` values in `apps/data-flow/kustomize/overlays/production/kustomization.yaml`

    ```bash
    images:
    - name: springcloud/spring-cloud-dataflow-server  # used for Kustomize matching
      newName: registry.example.com/example/spring-cloud-dataflow-server
      newTag: 2.5.1.RELEASE-jdbc
      configMapGenerator:
      - name: scdf-server
        files:
          - bootstrap.yaml
          - application.yaml
      bases:
        - ../../base
      patches:
      - deployment-patch.yaml
      - service-patch.yaml
    ```

If you add your own JDBC driver then you should initialize the schema for the database and turn off the database initialization of the skipper and data-flow apps as documented in the previous section.


### <a id='message-broker-configuration'></a> Message Broker Configuration

You can customize the configuration settings for the message broker in the `application.yaml` and the `deployment-path.yaml` files.
These files are preconfigured to connect to a RabbitMQ broker that can be installed either using the manifests provided in `services/dev/rabbitmq` or by installing the `bitnami/rabbitmq` Helm chart.

#### <a id='rabbit-broker-configuration'></a> Using RabbitMQ as the Message Broker

To use RabbitMQ as the message broker you need to review the settings for the `skipper` app provided in the Kustomization overlays for `dev` and `production`.

As mentioned above, the configuration is preconfigured to connect to a RabbitMQ broker running in the same namespace where you install Spring Cloud Data Flow for Kubernetes.

If you RabbitMQ broker runs in a different namespace or if you use an external broker then you need to remove references for the `rabbitmq` secret from `volume` and `container.volumeMounts` sections in the `deployment-patch.yaml` file in `dev` or `production` overlays.

The settings for connecting to the RabbitMQ broker are located in the `application.yaml` file for the `skipper` app in both `dev` and `production` overlays.
What is specified on the `environmentVariables` line will be passed to the Spring Cloud Stream apps that will be deployed. 

The configuration needs to include `SPRING_RABBITMQ_HOST`, `SPRING_RABBITMQ_PORT`, `SPRING_RABBITMQ_USERNAME` and `SPRING_RABBITMQ_PASSWORD` variables.
If the RabbitMQ cluster is running in the same namespace where you install Spring Cloud Data Flow for Kubernetes then you can reference the service environment variables provided by Kubernetes.
If not, then you would have to replace these references with the actual values.

```yaml
spring:
  cloud:
    skipper:
      server:
        platform:
          kubernetes:
            accounts:
              default:
                environmentVariables: 'SPRING_CLOUD_CONFIG_ENABLED=false,SPRING_RABBITMQ_HOST=${RABBITMQ_SERVICE_HOST},SPRING_RABBITMQ_PORT=${RABBITMQ_SERVICE_PORT_AMQP},SPRING_RABBITMQ_USERNAME=user,SPRING_RABBITMQ_PASSWORD=${rabbitmq-password}'
```

<div class="note">
The <code>SPRING_CLOUD_CONFIG_ENABLED=false</code> setting avoids having each stream app attempting to look up a Spring Cloud Configuration Server instance.</div>

#### <a id='kafka-broker-configuration'></a> Using Kafka as the Message Broker

To use Kafka as the message broker you need to modify the settings for the `skipper` app provided in the Kustomization overlays for `dev` and `production`.
Remove references for the `rabbitmq` secret from `volume` and `container.volumeMounts` sections in the `deployment-patch.yaml` file.

The settings for connecting to the Kafka broker are located in the `application.yaml` file for the `skipper` app in both `dev` and `production` overlays.

You can provide your own Kafka cluster or install one using the `bitnami/kafka` Helm chart. In the following example we assume that Kafka was installed using the Helm chart mentioned. If you use a different Kafka installation you need to adjust the environment variables specified in the `application.yaml` file.

Remove or comment out the line with `environmentVariables` for RabbitMQ settings and uncomment the line with corresponding Kafka settings.
What is specified on the `environmentVariables` line will be passed to the Spring Cloud Stream apps that will be deployed. 

The configuration needs to include `SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS` and `SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES` variables with values for host and port for Kafka broker and Zookeeper respectivly.

If the Kafka cluster is running in the same namespace where you install Spring Cloud Data Flow for Kubernetes then you can reference the service environment variables provided by Kubernetes.
If not, then you would have to replace these references with the actual values.

```yaml
spring:
  cloud:
    skipper:
      server:
        platform:
          kubernetes:
            accounts:
              default:
                environmentVariables: 'SPRING_CLOUD_CONFIG_ENABLED=false,SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=${KAFKA_SERVICE_HOST}:${KAFKA_SERVICE_PORT},SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES=${KAFKA_ZOOKEEPER_SERVICE_HOST}:${KAFKA_ZOOKEEPER_SERVICE_PORT}'
```

<div class="note">
The <code>SPRING_CLOUD_CONFIG_ENABLED=false</code> setting avoids having each stream app attempting to look up a Spring Cloud Configuration Server instance.</div>

### <a id='ingress-resource-configuration'></a> Ingress Resource

You can modify the `ingress-patch.yaml` in the `apps/ingress/kustomize/overlays/dev/` or the `apps/ingress/kustomize/overlays/production/` directory.
Modify the host field to provide the DNS name required for your Ingress Controller configuration.

If your DNS name is `data-flow.example.com` then add that DNS name as the host for the entry under `spec.rules`:

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: scdf-ingress
spec:
  rules:
  - host: data-flow.example.com
    http:
      paths:
      - backend:
          serviceName: scdf-server
          servicePort: 80
        path: /
```

### <a id='loadbalancer-configuration'></a> Load Balancer

If your Kubernetes cluster supports Kubernetes Services of type LoadBalancer, you may use that type of service to provision a load balancer using an Ingress Controller such as NGINX or Contour.

<p class="note">
  This option is recommended when deploying Spring Cloud Data Flow for Kubernetes to VMware Enterprise PKS on AWS, Azure, or GCP, or on vSphere with NSX-T container networking.
</p>
<p class="note warning">
  This option is <strong>not compatible</strong> with VMware Enterprise PKS to vSphere with Flannel container networking.
</p>

### <a id='security-configuration'></a> Security

Spring Cloud Data Flow and Skipper servers use the Spring Security library to configure Authentication and Authorization using OAuth 2.0 and OpenID Connect.
This lets you integrate Spring Cloud Data Flow into Single Sign On (SSO) environments.
To configure the servers changes must be made to each server's `application.yaml` configuration file.
The [Spring Cloud Data Flow Reference Guide's Security section](https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#configuration-local-security) provides the overall introduction of how to configure the Data Flow Server and the [Spring Cloud Data Flow Reference Gude's Azure section](https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#appendix-identity-provider-azure) provides the instructions for configuration with Azure AD.
While the documentation is in the Spring Cloud Data Flow reference guide, the same steps must be take for the configuration of Skipper's `application.yaml` file.

### <a id='container-registry'></a> Container Registry

The Data Flow server displays Spring Boot application metadata that is stored as a label in the container image.
To retrieve that metadata configure the Data Flow server to access the container registry for the application images you will be deploying.
The section in the `application.yaml` file where this configuration goes is under the root

```
spring:
  cloud:
    dataflow:
      container:
        registry-configurations:
```
The examples of how to configure this section for Docker Hub, Artifactory/JFrog, Amazon Elastic Container Registry, Azure Container Registry, and the Harbor registry are in [Spring Cloud Data Flow Reference Guide's Container Metadata section](https://dataflow.spring.io/docs/feature-guides/general/application-metadata/#using-metadata-container-image-labels)


### <a id='monitoring'></a> Monitoring
In order to support both short live and long lived application monitoring, task and stream applications deployed Data Flow and Skipper can use the [Prometheus RSocket Proxy](https://github.com/micrometer-metrics/prometheus-rsocket-proxy).
Other monitoring technologies that use the Micrometer library are supported as well but their configuration will be different than the example shown below.
The Data Flow Dashboard provides a convenient link to custom [Stream and Task Grafana dashboards](https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/src/grafana/prometheus/docker/grafana) that you can install into your Grafana server.


As an example of configuring monitoring for all deployed tasks and stream applications using Prometheus via an RSocket gateway, the following section in Data Flow's `application.yaml` should be configured.

```
spring:
  cloud:
    dataflow:
      applicationProperties:
        stream:
          management:
            metrics:
              export:
                prometheus:
                  enabled: true
                  rsocket:
                    enabled: true
                    host: prometheus-proxy
                    port: 7001
        task:
          management:
            metrics:
              export:
                prometheus:
                  enabled: true
                  rsocket:
                    enabled: true
                    host: prometheus-proxy
                    port: 7001
      grafana-info:
        url: 'https://grafana:3000'
```

The Grafana `url` should and Prometheus host/port values should be changed for you specific environment.
The Data Flow microsite contains more information on [stream monitoring](https://dataflow.spring.io/docs/feature-guides/streams/monitoring/#kubernetes) and [task monitoring](https://dataflow.spring.io/docs/feature-guides/batch/monitoring/#prometheus-1).

Since these common application properties are passed to the deployed task and stream applications, if you are using a different monitoring infrastrure that is supported by the micrometer library, you can set the appropriate Spring Boot configuration values for yoru monitoring infrastructure underneath the `applicationProperties.stream` and `applicationProperties.task` sections above.


## <a id='next-installing-scdf'></a> Next: Installing SCDF for Kubernetes

Proceed to the [Installing SCDF for Kubernetes](installing-scdf-for-kubernetes.html) topic for installation instructions.