---
title: Configuring Installation Values for Spring Cloud Data Flow for Kubernetes
owner: Spring Cloud Data Flow Release Engineering
---

This topic describes how to configure the Spring Cloud Data Flow for Kubernetes installation resources before deploying to the Kubernetes cluster.

Before proceeding, review the [Preparing to Install SCDF for Kubernetes](preparing-to-install-scdf-for-kubernetes.html) topic for details about:

* Preparing the Spring Cloud Data Flow for Kubernetes installation resources on your local workstation
* Installing the required command-line tools

## <a id='overview'></a> Overview

A few concepts drive how the configuration of Spring Cloud Data Flow for Kubernetes is performed:

* Support for multiple deployment environments
* Configuration of application properties
* Configuration of Kubernetes Resources
* Support for installation in an air-gapped environment

The following sections explain these concepts and the steps you need to take to configure them.


## <a id='support-for-multiple-environments'></a> Support for Multiple Deployment Environments

The installation is based on the [Kustomize](https://kustomize.io) tool, which lets you customize Kubernetes resources by providing base resources files and patch files that modify the base resources to target different deployment environments.
Kustomize is built into the kubectl CLI.
The provided configuration files support a development and a production environment.
The development environment is intended to let you quickly try Spring Cloud Data Flow for Kubernetes by provisioning a RabbitMQ message broker and a PostgreSQL database.


### <a id='directory-structure'></a> Directory Structure

The installation directory structure contains the following directories:

* `bin`: Scripts for installing to the development environment as well as a script for performing image relocation
* `apps`: Kubernetes resource files and application configuration files to support deployment to multiple environments
* `services`: Kubernetes resources for deploying RabbitMQ and PostgreSQL for use with the development environment

The apps directory contains the following folder structure based on Kustomize namving conventions

<pre>
.
├── apps
│   ├── data-flow
│   │   ├── images
│   │   ├── kustomize
│   │   │   ├── base
│   │   │   └── overlays
│   │   └── schemas
│   ├── ingress
│   │   └── kustomize
│   │       ├── base
│   │       └── overlays
│   └── skipper
│       ├── images
│       ├── kustomize
│       │   ├── base
│       │   └── overlays
│       └── schemas
</pre>

There are two directories, one for each application that is part of Spring Cloud Data Flow for Kubernetes.
The first directory, `data-flow`, contains the Data Flow Server, and the second directory, `skipper`, contains the Skipper server.
There is also an `ingress` directory that contains configuration files for an Ingress resource.
Within each application directory there is the following directory structure

* `images`: The location for container images (only present if "SCDF for Kubernetes installation images" archive was downloaded and extracted)
* `kustomize`: Directory containing Kubernetes and application configuration files for use with Kustomize.
* `schemas`: Database schemas that you can install manually if you do not want each server to install them upon startup.

The contents of the kustomize directory are shown below.
Edit files in this directory to configure the application and Kubernetes resources.

<pre>
.
├── base
│   ├── deployment.yaml
│   ├── kustomization.yaml
│   ├── role-binding.yaml
│   ├── roles.yaml
│   ├── service-account.yaml
│   └── service.yaml
└── overlays
    ├── dev
    │   ├── application.yaml
    │   ├── deployment-patch.yaml
    │   ├── kustomization.yaml
    │   └── service-patch.yaml
    └── production
        ├── application.yaml
        ├── deployment-patch.yaml
        ├── kustomization.yaml
        └── service-patch.yaml
</pre>

**Base**

The `base` directory contains kubernetes resource files with default configuration values that are then patched by files in the overlays directory.
The file `kustomization.yaml` references the other YAML files in that directory.

**Overlays**

There is a directory per deployment environment -- in this case, `dev` and `production` environments.
In each directory, a `kustomization.yaml` file references the `kustomization.yaml` file in the base and adds additional patches to modify values unique to the target deployment environment.
You can also add additonal configuration in the `kustomization.yaml` file to set the target namespace to deploy and to add labels and name prefixes. See the [kustomization documentation](https://github.com/kubernetes-sigs/kustomize/blob/master/docs/fields.md) for additional fields that can be configured in the kustomization.yaml file.


## <a id='configuration-steps'></a> Configuration Steps

The `application.yaml` file is a Spring Boot configuration file that you can edit to configure the Data Flow and Skipper server.
For example, you can configure `spring.datasource` properties or `spring.cloud.dataflow.features` properties.
The `deployment-patch.yaml` and `service-patch.yaml` files are where you can patch the base kubernetes resources with values appropriate for your target environment.
Examples of these fields are memory/cpu allocations and ingress configuration.


### <a id='database-configuration'></a> Database Configuration

You can customize the database configuration settings in the `application.yaml` and `deployment-patch.yaml` files.
These files are preconfigured to connect to a PostgreSQL database that can be installed either by using the manifests provided in `services/dev/postgresql` or by installing the `bitnami/postgresql` Helm chart.

#### <a id='database-connection'></a> Database Connections Settings

To use a different database, you need to replace or remove the secret `volume` and `volumeMount` in the `deployment-patch.yaml` file.
If you replace the secret, we recommend that you provide `database-password` as the path for the password key, since doing so simplifies the changes needed in the `application.yaml` file.

You need to change the database settings for both the `skipper` and the `data-flow` applications.
The following example uses a secret named `production-db` with a `password` key (you can specify additional keys if they are available in the secret and reference the paths you specify in `application.yaml`):

```yaml
      containers:
      - name: skipper
        volumeMounts:
          - name: database
            mountPath: /workspace/runtime/secrets/database
            readOnly: true
  ...

      volumes:
        - name: database
          secret:
            secretName: postgresql
            items:
            - key: postgresql-password
              path: database-password
```

The settings in `application.yaml` that you need to consider are under `spring.datasource`.
Modify the `url`, `username`, `password` and `driverClassName` values to match you detabase settings:

```yaml
spring:

  ...

  datasource:
    url: jdbc:postgresql://database-host:5432/dataflow-db
    username: postgres
    password: ${database-password}
    driverClassName: org.postgresql.Driver
    testOnBorrow: true
    validationQuery: "SELECT 1"
```

#### <a id='database-connection'></a> Initializing the Database Schema

You can initialize your database schema by using the DDL scripts available under `apps/skipper/schemas` and `apps/data-flow/schemas`.
We provide DDL for DB2, MySQL, Oracle, PostgreSQL, and MS SQL Server.
If there are multiple files, they must be applied in the correct order, starting with `V1-` followed by `V2-` and `V3-` and so on.

If you initialize the schema for the database, you can turn off the database initialization of the skipper and data-flow apps by adding the following settings to the respective `application.yaml` files:

```yaml
spring:

  ...

  jpa:
    hibernate:
      ddlAuto: none
  flyway:
    enabled: false
```

#### <a id='database-connection'></a> Adding JDBC Driver

It is possibe to add your own JDBC driver if you do not want to use the JDBC drivers that are provided.
JDBC drivers for MySQL (via MariaDB driver), HSQLDB, PostgreSQL along with embedded H2 are provided in the published application artifacts.

In order to add a JDBC driver, you need to perform the following steps:

1. Download the `SCDF Pro Server jar file` release artifact from [VMware Tanzu Network](https://network.pivotal.io/products/p-scdf-for-kubernetes).

1. Download the server jars for `skipper` and `composed-task-runner`:

    ```bash
    wget https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-skipper-server/2.6.0/spring-cloud-skipper-server-2.6.0.jar
    wget https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-dataflow-composed-task-runner/2.7.0/spring-cloud-dataflow-composed-task-runner-2.7.0.jar
    ```

1. Create a directory containing the JDBC driver to be added:

    ```bash
    mkdir -p BOOT-INF/lib
    ```

1. Copy your JDBC driver jar to this directory.

1. Add the driver to the downloaded jars:

    ```bash
    jar -u0f spring-cloud-skipper-server-2.6.0.jar BOOT-INF/lib/*.jar
    jar -u0f scdf-pro-server-1.1.0.jar BOOT-INF/lib/*.jar
    jar -u0f spring-cloud-dataflow-composed-task-runner-2.7.0.jar BOOT-INF/lib/*.jar
    ```

1. Create new container images using the updated server jars. 

    You should use a base image based on JDK 8.
    The easiest way to build new container images is to use the [pack CLI utility](https://buildpacks.io/docs/install-pack/).
    Create a `registry_prefix` environment variable containing the prefix you want to use.

    Here is an example:

    ```
    export registry_prefix=registry.example.com/example
    ```

    Now, build the three container images.

    For the `skipper` app run:

    ```
    pack build --env BP_JVM_VERSION=8 \
      --path spring-cloud-skipper-server-2.6.0.jar \
      --builder gcr.io/paketo-buildpacks/builder:base \
      --publish ${registry_prefix}/spring-cloud-skipper-server:2.6.0-jdbc
    ```

    For the `data-flow` app run:
    
    ```
    pack build --env BP_JVM_VERSION=8 \
      --path scdf-pro-server-1.1.0.jar \
      --builder gcr.io/paketo-buildpacks/builder:base \
      --publish ${registry_prefix}/scdf-pro-server:1.1.0-jdbc
    ```

    For the `composed-task-runner` app run:
    
    ```
    pack build --env BP_JVM_VERSION=8 \
      --path spring-cloud-dataflow-composed-task-runner-2.7.0.jar \
      --builder gcr.io/paketo-buildpacks/builder:base \
      --publish ${registry_prefix}/spring-cloud-dataflow-composed-task-runner:2.7.0-jdbc
    ```

1. Update your application kustomization manifests with the new image locations:

    For the `skipper` application, modify `newName` and `newTag` values in `apps/skipper/kustomize/overlays/production/kustomization.yaml`. Also, either remove the `digest` or update it with the new image digest.

    ```bash
    images:
    - name: springcloud/spring-cloud-skipper-server  # used for Kustomize matching
      newName: registry.example.com/example/spring-cloud-skipper-server
      newTag: 2.6.0-jdbc
    configMapGenerator:
    - name: skipper
      files:
        - bootstrap.yaml
        - application.yaml
    bases:
      - ../../base
    patches:
    - deployment-patch.yaml
    - service-patch.yaml
    ```

    For the `data-flow` application, modify `newName` and `newTag` values in `apps/data-flow/kustomize/overlays/production/kustomization.yaml`. Also, either remove the `digest` or update it with the new image digest.

    ```bash
    images:
    - name: springcloud/spring-cloud-dataflow-server  # used for Kustomize matching
      newName: registry.example.com/example/scdf-pro-server
      newTag: 1.1.0-jdbc
      configMapGenerator:
      - name: scdf-server
        files:
          - bootstrap.yaml
          - application.yaml
      bases:
        - ../../base
      patches:
      - deployment-patch.yaml
      - service-patch.yaml
    ```

    For the `composed-task-runner` image you need to modify the `apps/data-flow/kustomize/overlays/production/application.yaml` file.
    Change the `spring.cloud.dataflow.task.composedTaskRunnerUri` entry to match your new image. The value should be prefixed with `docker://`.
    You can either provide a tag or a digest that matches your new image. Here is an example:

    ```yaml
     spring:
      cloud:
        config:
          enabled: false
        dataflow:
          server:
            uri: http://${SCDF_SERVER_SERVICE_HOST}:${SCDF_SERVER_SERVICE_PORT}
          features:
            schedules-enabled: true
          task:
            composedTaskRunnerUri: docker://registry.example.com/my-project/spring-cloud-dataflow-composed-task-runner:2.7.0-jdbc

    ...

    ```

If you add your own JDBC driver, you should initialize the schema for the database and turn off the database initialization of the skipper and data-flow applications as documented in the previous section.


### <a id='message-broker-configuration'></a> Message Broker Configuration

You can customize the configuration settings for the message broker in the `application.yaml` and the `deployment-patch.yaml` files.
These files are preconfigured to connect to a RabbitMQ broker that can be installed either by using the manifests provided in `services/dev/rabbitmq` or by installing the `bitnami/rabbitmq` Helm chart.

#### <a id='rabbit-broker-configuration'></a> Using RabbitMQ as the Message Broker

To use RabbitMQ as the message broker, you need to review the settings for the `skipper` application provided in the Kustomization overlays for `dev` and `production`.

As mentioned earlier, the configuration is preconfigured to connect to a RabbitMQ broker running in the same namespace in which you installed Spring Cloud Data Flow for Kubernetes.

If your RabbitMQ broker runs in a different namespace or if you use an external broker, you need to remove references for the `rabbitmq` secret from `volume` and `container.volumeMounts` sections in the `deployment-patch.yaml` file in `dev` or `production` overlays.

The settings for connecting to the RabbitMQ broker are located in the `application.yaml` file for the `skipper` app in both `dev` and `production` overlays.
What is specified on the `environmentVariables` line is passed to the Spring Cloud Stream apps that are deployed.

The configuration needs to include the `SPRING_RABBITMQ_HOST`, `SPRING_RABBITMQ_PORT`, `SPRING_RABBITMQ_USERNAME`, and `SPRING_RABBITMQ_PASSWORD` variables.
If the RabbitMQ cluster runs in the same namespace where you install Spring Cloud Data Flow for Kubernetes, you can reference the service environment variables provided by Kubernetes.
If not, then you would have to replace these references with the actual values:

```yaml
spring:
  cloud:
    skipper:
      server:
        platform:
          kubernetes:
            accounts:
              default:
                environmentVariables: 'SPRING_CLOUD_CONFIG_ENABLED=false,SPRING_RABBITMQ_HOST=${RABBITMQ_SERVICE_HOST},SPRING_RABBITMQ_PORT=${RABBITMQ_SERVICE_PORT_AMQP},SPRING_RABBITMQ_USERNAME=user,SPRING_RABBITMQ_PASSWORD=${rabbitmq-password}'
```

<div class="note">
The <code>SPRING_CLOUD_CONFIG_ENABLED=false</code> setting avoids having each stream application try to look up a Spring Cloud Configuration Server instance.</div>

#### <a id='kafka-broker-configuration'></a> Using Kafka as the Message Broker

To use Kafka as the message broker, you need to modify the settings for the `skipper` application provided in the Kustomization overlays for `dev` and `production`.
Remove the references for the `rabbitmq` secret from `volume` and `container.volumeMounts` sections in the `deployment-patch.yaml` file.

The settings for connecting to the Kafka broker are located in the `application.yaml` file for the `skipper` application in both the `dev` and `production` overlays.

You can provide your own Kafka cluster or install one by using the `bitnami/kafka` Helm chart. In the following example, we assume that Kafka was installed by using the Helm chart mentioned. If you use a different Kafka installation, you need to adjust the environment variables specified in the `application.yaml` file.

Remove or comment out the line with `environmentVariables` for RabbitMQ settings and uncomment the line with the corresponding Kafka settings.
What is specified on the `environmentVariables` line is passed to the Spring Cloud Stream apps that are deployed.

The configuration needs to include the `SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS` and `SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES` variables with values for host and port for Kafka broker and Zookeeper respectivly.

If the Kafka cluster is running in the same namespace in which you install Spring Cloud Data Flow for Kubernetes, you can reference the service environment variables provided by Kubernetes.
If not, you would have to replace these references with the actual values.

```yaml
spring:
  cloud:
    skipper:
      server:
        platform:
          kubernetes:
            accounts:
              default:
                environmentVariables: 'SPRING_CLOUD_CONFIG_ENABLED=false,SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=${KAFKA_SERVICE_HOST}:${KAFKA_SERVICE_PORT},SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES=${KAFKA_ZOOKEEPER_SERVICE_HOST}:${KAFKA_ZOOKEEPER_SERVICE_PORT}'
```

<div class="note">
The <code>SPRING_CLOUD_CONFIG_ENABLED=false</code> setting avoids having each stream application try to look up a Spring Cloud Configuration Server instance.</div>

### <a id='ingress-resource-configuration'></a> Ingress Resource

If your Kubernetes cluster supports Kubernetes Services of type `LoadBalancer`, you may use that type of service to provision a load balancer by using an Ingress Controller such as NGINX or Contour.

If your cluster has an Ingress Controller configured, you can use an Ingress resource manager to manage external access to the Data Flow server service.

You can modify the `ingress-patch.yaml` file in either `apps/ingress/kustomize/overlays/dev/` or `apps/ingress/kustomize/overlays/production/`.
Modify the host field to provide the DNS name required for your Ingress Controller configuration.

If your DNS name is `data-flow.example.com`, add that DNS name as the host for the entry under `spec.rules`:

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: scdf-ingress
spec:
  rules:
  - host: data-flow.example.com
    http:
      paths:
      - backend:
          serviceName: scdf-server
          servicePort: 80
        path: /
```

### <a id='security-configuration'></a> Security

The Spring Cloud Data Flow and Skipper servers use the Spring Security library to configure authentication and authorization by using OAuth 2.0 and OpenID Connect.
This lets you integrate Spring Cloud Data Flow into Single Sign On (SSO) environments.
To configure the servers, change each server's `application.yaml` configuration file.
The [Spring Cloud Data Flow Reference Guide's Security section](https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#configuration-local-security) describes how to configure the Data Flow Server, and the [Spring Cloud Data Flow Reference Gude's Azure section](https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#appendix-identity-provider-azure) describes configuration with Azure AD.
While the documentation is in the Spring Cloud Data Flow reference guide, the same steps must be take for the configuration of Skipper's `application.yaml` file.

### <a id='container-registry'></a> Container Registry

The Data Flow server displays Spring Boot application metadata that is stored as a label in the container image.
To retrieve that metadata, configure the Data Flow server to access the container registry for the application images you will be deploying.
This configuration goes under the following section in the `application.yaml` file:

```
spring:
  cloud:
    dataflow:
      container:
        registry-configurations:
```
The examples of how to configure this section for Docker Hub, Artifactory/JFrog, Amazon Elastic Container Registry, Azure Container Registry, and the Harbor registry are in the Spring Cloud Data Flow Reference Guide's [Container Metadata](https://dataflow.spring.io/docs/feature-guides/general/application-metadata/#using-metadata-container-image-labels) section.

### <a id='imagePullSecrets'></a> ImagePullSecrets for Apps

When launching task and stream apps the Kubernetes nodes need to be able to pull the corresponding container image.
If the image is stored in a private repository we need to provide authentication information for the nodes to pull images from a registry.

This authentication information is provided using a secret.
In the namespace where you intend to install, create a secret, as shown in the next example.

First, set the username and password variables based on your login credentials for the registry containig the images for the apps.

```bash
registry=<your registry host>
username=<your user name>
password=<your password>
```

Then create the secret using the values you set in the preceding example:

```bash
kubectl create secret \
docker-registry scdf-apps-regcred \
--docker-server=${registry} \
--docker-username=${username} \
--docker-password=${password}
```

For stream apps we add the name of this secret to the `application.yaml` file for the `skipper` application:

```yaml
spring:
  cloud:
    skipper:
      server:
        platform:
          kubernetes:
            accounts:
              default:
                imagePullSecret: scdf-apps-regcred
```

<div class="note">If you want to use the Composed Task Runner then you must create a secret with the credentials for the Tanzu Net Registry or, if you relocated the image for the Composed Task Runner, to the registry where it was relocated.</div>

For task apps we add the name of this secret to the `application.yaml` file for the `data-flow` application:

```yaml
spring:
  cloud:
    dataflow:
      task:
        platform:
          kubernetes:
            accounts:
              default:
                imagePullSecret: scdf-apps-regcred
```

### <a id='task-limiting'></a> Task Limiting

Spring Cloud Data Flow lets a user limit the maximum number of concurrently running tasks to prevent the saturation of IaaS/hardware resources.
The default limit is set to 20.
If the number of concurrently running tasks on a platform instance is greater than or equal to the limit, the next task launch request fails, and an error message is returned through the RESTful API, Shell, or UI.
This limit can be configured by setting the `maximumConcurrentTasks` property in the account:

```
spring:
  cloud:
    dataflow:
      task:
        platform:
          kubernetes:
            accounts:
              default:
                namespace: default
                maximumConcurrentTasks: 40
                limits:
                  memory: 1024Mi
```

Launching a task, if successful, results in a running pod that we expect to eventually complete or fail.
As there is no easy way to identify pods that correspond to a task, we count only pods that were launched by the Data Flow Server.
Since the task launcher provides the task's ID as a label in the pod’s metadata, we filter all running pods for the presence of this label.

## <a id='next-installing-scdf'></a> Next: Configuring Monitoring for SCDF for Kubernetes

Proceed to the [Configuring Monitoring for SCDF for Kubernetes](configuring-monitoring.html) topic for monitoring configuration instructions.
