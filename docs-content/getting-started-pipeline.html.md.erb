---
title: Creating a Data Pipeline with Spring Cloud Data Flow for Kubernetes
owner: Spring Cloud Data Flow Release Engineering
---

This topic describes how to get started using Spring Cloud Data Flow for Kubernetes.
The examples in this section show how to quickly create a data pipeline.

## <a id='start-shell'></a> Start the Spring Cloud Data Flow Shell

Before continuing with this topic, you need to download and connect the Spring Cloud Data Flow Shell, as described in the [Connecting to SCDF for Kubernetes](connecting-scdf-for-kubernetes.html) topic.

```bash
$ java -jar spring-cloud-dataflow-shell-2.5.3.RELEASE.jar --dataflow.uri=http://data-flow.example.com
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/

2.5.3.RELEASE

Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
Successfully targeted http://data-flow.example.com
dataflow:>
```

## <a id='import-stream-apps'></a> Import the Spring Cloud Stream Applications

Import the stream app starters using the Data Flow shell’s `app import` command.

Depending on the message broker that you are using, there are two versions of these apps that you can register.
One version is for "RabbitMQ + Docker", and the other version is for "Apache Kafka + Docker".
See the [Register Supported Applications and Tasks](https://docs.spring.io/spring-cloud-dataflow/docs/2.5.3.RELEASE/reference/htmlsingle/#supported-apps-and-tasks) page for more detail.

For RabbitMQ, use:

```bash
dataflow:>app import https://dataflow.spring.io/rabbitmq-docker-latest
Successfully registered 66 applications from [source.sftp.metadata, sink.throughput.metadata, processor.object-detection.metadata, sink.cassandra.metadata, source.loggregator.metadata, source.s3, processor.aggregator.metadata, sink.hdfs, sink.rabbit, sink.ftp.metadata, processor.tasklaunchrequest-transform.metadata, sink.pgcopy, processor.httpclient, sink.jdbc, source.tcp, source.s3.metadata, sink.jdbc.metadata, sink.mongodb.metadata, sink.tcp.metadata, source.mqtt, source.gemfire.metadata, sink.gemfire.metadata, source.load-generator.metadata, sink.log, sink.redis-pubsub, sink.task-launcher-dataflow, sink.pgcopy.metadata, processor.python-http.metadata, sink.counter.metadata, processor.grpc, processor.twitter-sentiment, sink.file.metadata, sink.s3.metadata, processor.python-http, processor.tcp-client, sink.hdfs.metadata, source.cdc-debezium.metadata, sink.sftp.metadata, sink.tcp, source.sftp, source.cdc-debezium, source.http, processor.groovy-filter.metadata, processor.splitter.metadata, source.syslog.metadata, processor.image-recognition, source.file, processor.bridge, processor.tensorflow, processor.tensorflow.metadata, sink.cassandra, processor.twitter-sentiment.metadata, processor.python-jython.metadata, source.time.metadata, source.tcp.metadata, source.sftp-dataflow.metadata, processor.transform.metadata, source.ftp.metadata, processor.scriptable-transform, source.triggertask.metadata, source.mqtt.metadata, processor.grpc.metadata, source.jms.metadata, source.syslog, source.file.metadata, processor.transform, source.time, processor.bridge.metadata, sink.s3, source.triggertask, source.gemfire-cq.metadata, source.trigger.metadata, source.jms, source.sftp-dataflow, source.mail, sink.mqtt.metadata, source.mongodb, source.rabbit, sink.router, source.ftp, sink.file, processor.groovy-transform.metadata, processor.counter.metadata, source.tcp-client, processor.scriptable-transform.metadata, processor.pose-estimation, processor.splitter, source.gemfire, sink.redis-pubsub.metadata, source.load-generator, source.loggregator, processor.aggregator, processor.groovy-transform, processor.object-detection, processor.python-jython, sink.throughput, processor.pose-estimation.metadata, sink.ftp, processor.filter.metadata, sink.mqtt, source.trigger, sink.gemfire, processor.header-enricher.metadata, sink.sftp, processor.filter, source.jdbc, source.gemfire-cq, source.twitterstream, sink.rabbit.metadata, sink.websocket.metadata, processor.httpclient.metadata, sink.log.metadata, processor.tasklaunchrequest-transform, processor.tcp-client.metadata, sink.websocket, processor.image-recognition.metadata, source.jdbc.metadata, source.mail.metadata, source.rabbit.metadata, source.tcp-client.metadata, processor.counter, processor.pmml, source.http.metadata, processor.groovy-filter, sink.counter, source.twitterstream.metadata, processor.header-enricher, sink.task-launcher-dataflow.metadata, source.mongodb.metadata, processor.pmml.metadata, sink.router.metadata, sink.mongodb]
```

For Apache Kafka, use:

```bash
dataflow:>app import https://dataflow.spring.io/kafka-docker-latest
Successfully registered 66 applications from [source.sftp.metadata, sink.throughput.metadata, processor.object-detection.metadata, sink.cassandra.metadata, source.loggregator.metadata, source.s3, processor.aggregator.metadata, sink.hdfs, sink.rabbit, sink.ftp.metadata, processor.tasklaunchrequest-transform.metadata, sink.pgcopy, processor.httpclient, sink.jdbc, source.tcp, source.s3.metadata, sink.jdbc.metadata, sink.mongodb.metadata, sink.tcp.metadata, source.mqtt, source.gemfire.metadata, sink.gemfire.metadata, source.load-generator.metadata, sink.log, sink.redis-pubsub, sink.task-launcher-dataflow, sink.pgcopy.metadata, processor.python-http.metadata, sink.counter.metadata, processor.grpc, processor.twitter-sentiment, sink.file.metadata, sink.s3.metadata, processor.python-http, processor.tcp-client, sink.hdfs.metadata, source.cdc-debezium.metadata, sink.sftp.metadata, sink.tcp, source.sftp, source.cdc-debezium, source.http, processor.groovy-filter.metadata, processor.splitter.metadata, source.syslog.metadata, processor.image-recognition, source.file, processor.bridge, processor.tensorflow, processor.tensorflow.metadata, sink.cassandra, processor.twitter-sentiment.metadata, processor.python-jython.metadata, source.time.metadata, source.tcp.metadata, source.sftp-dataflow.metadata, processor.transform.metadata, source.ftp.metadata, processor.scriptable-transform, source.triggertask.metadata, source.mqtt.metadata, processor.grpc.metadata, source.jms.metadata, source.syslog, source.file.metadata, processor.transform, source.time, processor.bridge.metadata, sink.s3, source.triggertask, source.gemfire-cq.metadata, source.trigger.metadata, source.jms, source.sftp-dataflow, source.mail, sink.mqtt.metadata, source.mongodb, source.rabbit, sink.router, source.ftp, sink.file, processor.groovy-transform.metadata, processor.counter.metadata, source.tcp-client, processor.scriptable-transform.metadata, processor.pose-estimation, processor.splitter, source.gemfire, sink.redis-pubsub.metadata, source.load-generator, source.loggregator, processor.aggregator, processor.groovy-transform, processor.object-detection, processor.python-jython, sink.throughput, processor.pose-estimation.metadata, sink.ftp, processor.filter.metadata, sink.mqtt, source.trigger, sink.gemfire, processor.header-enricher.metadata, sink.sftp, processor.filter, source.jdbc, source.gemfire-cq, source.twitterstream, sink.rabbit.metadata, sink.websocket.metadata, processor.httpclient.metadata, sink.log.metadata, processor.tasklaunchrequest-transform, processor.tcp-client.metadata, sink.websocket, processor.image-recognition.metadata, source.jdbc.metadata, source.mail.metadata, source.rabbit.metadata, source.tcp-client.metadata, processor.counter, processor.pmml, source.http.metadata, processor.groovy-filter, sink.counter, source.twitterstream.metadata, processor.header-enricher, sink.task-launcher-dataflow.metadata, source.mongodb.metadata, processor.pmml.metadata, sink.router.metadata, sink.mongodb]
```

## <a id='create-stream'></a> Create the Stream Definition

With the app starters imported, you can use three apps (the `http` source, the `split` processor, and the `log` sink) to create a stream that consumes data through an HTTP POST request, processes it by splitting it into words, and outputs the results in logs.

Create the stream by using the Data Flow shell’s `stream create` command:

```bash
dataflow:>stream create --name words --definition "http | splitter --expression=payload.split(' ') | log"
Created new stream 'words'
```

## <a id='deploy-stream'></a> Deploy the Stream

Next, deploy the stream by using the `stream deploy` command:

```bash
dataflow:>stream deploy words --properties deployer.http.kubernetes.createLoadBalancer=true
Deployment request has been sent for stream 'words'
```

In order to be able to post HTTP requests to the `http` app, we need to specify a deployment property requesting a `LoadBalancer` for the apps service.

You can run the `kubectl get pods` command from a different terminal window to see the application pods deployed as part of the stream:

```bash
$ kubectl get pods -l role=spring-app
NAME                                 READY   STATUS    RESTARTS   AGE
words-http-v1-7c977c4965-5q27m       1/1     Running   0          5m8s
words-log-v1-855f6ddd69-slmnn        1/1     Running   0          5m8s
words-splitter-v1-5466fdf6d4-lrz2c   1/1     Running   0          5m8s
```

You can also check the stream status from the shell by using the `stream list` command:

```bash
dataflow:>stream list
╔═══════════╤═══════════╤═══════════════════════════════════════════════════════╤═════════════════════════════════════════╗
║Stream Name│Description│                   Stream Definition                   │                 Status                  ║
╠═══════════╪═══════════╪═══════════════════════════════════════════════════════╪═════════════════════════════════════════╣
║words      │           │http | splitter --expression="payload.split(' ')" | log│The stream has been successfully deployed║
╚═══════════╧═══════════╧═══════════════════════════════════════════════════════╧═════════════════════════════════════════╝
```

## <a id='using-pipeline'></a> Using the Deployed Data Pipeline

To view the log output from the `log` sink, you can run the following command in a new terminal window:

```bash
kubectl logs -f deployment/words-log-v1
```
Next, we need to look up the IP address that was assigned to the `http` apps service resource:

```bash
$ kubectl get service words-http-v1
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)          AGE
words-http-v1   LoadBalancer   10.47.244.113   104.197.218.242   8080:31383/TCP   4m4s
```

We use the IP address listed as the `EXTERNAL-IP` and the port `8080` to access the app.
From the Data Flow shell send a POST request to the `words-http-v1` application using the IP address we just looked up:

```
dataflow:>http post --target http://104.197.218.242:8080 --data "This is a test"
> POST (text/plain) http://104.197.218.242:8080 This is a test
> 202 ACCEPTED
```

Watch for the processed data in the `words-log-v1` application’s logs:

```bash
2020-06-03 19:56:59.163  INFO 1 --- [container-0-C-1] log-sink                                 : This
2020-06-03 19:56:59.169  INFO 1 --- [container-0-C-1] log-sink                                 : is
2020-06-03 19:56:59.171  INFO 1 --- [container-0-C-1] log-sink                                 : a
2020-06-03 19:56:59.255  INFO 1 --- [container-0-C-1] log-sink                                 : test
```

## <a id='destroy-pipeline'></a> Destroying the Data Pipeline

First, undeploy the stream:

```bash
dataflow:>stream undeploy words
Un-deployed stream 'words'
```

Next, destroy the stream definition:

```bash
dataflow:>stream destroy words
Destroyed stream 'words'
```

## <a id='leran -more'></a> Learning More About Creating Streams and Tasks

To learn more about creating streams, see the [Getting Started with Stream Processing](https://dataflow.spring.io/docs/stream-developer-guides/getting-started/stream/) guide.

To learn about creating tasks, see the [Getting Started with Task Processing](https://dataflow.spring.io/docs/batch-developer-guides/getting-started/task/) guide.
